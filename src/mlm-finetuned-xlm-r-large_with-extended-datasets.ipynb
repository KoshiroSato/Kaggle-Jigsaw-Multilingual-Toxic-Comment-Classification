{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/riblidezso/train-from-mlm-finetuned-xlm-roberta-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, emoji, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import transformers\n",
    "from transformers import TFRobertaModel, AutoTokenizer\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 192 \n",
    "BATCH_SIZE = 16 \n",
    "TOTAL_STEPS_STAGE1 = 2000\n",
    "VALIDATE_EVERY_STAGE1 = 200\n",
    "TOTAL_STEPS_STAGE2 = 200\n",
    "VALIDATE_EVERY_STAGE2 = 10\n",
    "\n",
    "### Different learning rate for transformer and head ###\n",
    "LR_TRANSFORMER = 5e-6\n",
    "LR_HEAD = 1e-3\n",
    "\n",
    "PRETRAINED_TOKENIZER=  'jplu/tf-xlm-roberta-large'\n",
    "PRETRAINED_MODEL = '../input/fine-tuned-model'\n",
    "D = '../input/jigsaw-multilingual-toxic-comment-classification/'\n",
    "D_TRANS = '../input/jigsaw-train-multilingual-coments-google-api/'\n",
    "EX_D = '../input/toxic-comment-detection-multilingual-extended/archive/italian/'\n",
    "\n",
    "\n",
    "# no extensive logging \n",
    "logging.getLogger().setLevel(logging.NOTSET)\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed):\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_TPU():\n",
    "    \"\"\"Detect hardware, return appropriate distribution strategy\"\"\"\n",
    "    try:\n",
    "        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "        # set: this is always the case on Kaggle.\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    else:\n",
    "        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "\n",
    "    global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "    return tpu, strategy, global_batch_size\n",
    "\n",
    "\n",
    "tpu, strategy, global_batch_size = connect_to_TPU()\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended datasets\n",
    "it_df_1 = pd.read_csv(EX_D+'haspeede_TW-train.tsv', delimiter='\\t').rename(columns={'comment':'comment_text'})\n",
    "it_df_2 = pd.read_csv(EX_D+'haspeede_FB-train.tsv', delimiter='\\t').rename(columns={'comment':'comment_text'})\n",
    "\n",
    "# https://www.kaggle.com/raenish/cheatsheet-text-helper-functions\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_nonalp(text):\n",
    "    line = re.findall(\"[^A-Za-z0-9 ]\",text)\n",
    "    for i in range(len(line)):\n",
    "        text = text.replace(line[i], '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def apply_remove_func(df):\n",
    "    for i in range(len(df)):\n",
    "        df['comment_text'][i] = remove_nonalp(remove_emoji(df['comment_text'][i]))\n",
    "    return df\n",
    "\n",
    "it_df_1 = apply_remove_func(it_df_1)\n",
    "it_df_2 = apply_remove_func(it_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jigsaw_trans(langs=['tr','it','es','ru','fr','pt'], \n",
    "                      columns=['comment_text', 'toxic']):\n",
    "    train_6langs=[]\n",
    "    for i in range(len(langs)):\n",
    "        fn = D_TRANS+'jigsaw-toxic-comment-train-google-%s-cleaned.csv'%langs[i]\n",
    "        train_6langs.append(downsample(pd.read_csv(fn)[columns]))\n",
    "        \n",
    "    train_6langs.append(downsample(it_df_1[columns]))\n",
    "    train_6langs.append(downsample(it_df_2[columns]))\n",
    "    return train_6langs\n",
    "\n",
    "def downsample(df):\n",
    "    \"\"\"Subsample the train dataframe to 50%-50%\"\"\"\n",
    "    ds_df= pd.concat([\n",
    "        df.query('toxic==1'),\n",
    "        df.query('toxic==0').sample(sum(df.toxic))\n",
    "    ])\n",
    "    \n",
    "    return ds_df\n",
    "    \n",
    "\n",
    "train_df = pd.concat(load_jigsaw_trans()) \n",
    "val_df = pd.read_csv(D+'validation.csv')\n",
    "test_df = pd.read_csv(D+'test.csv')\n",
    "sub_df = pd.read_csv(D+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])\n",
    "    \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_TOKENIZER)\n",
    "X_train = regular_encode(train_df.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "X_val = regular_encode(val_df.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "X_test = regular_encode(test_df.content.values, tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = train_df.toxic.values.reshape(-1,1)\n",
    "y_val = val_df.toxic.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dist_dataset(X, y=None, training=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "    ### Add y if present ###\n",
    "    if y is not None:\n",
    "        dataset_y = tf.data.Dataset.from_tensor_slices(y)\n",
    "        dataset = tf.data.Dataset.zip((dataset, dataset_y))\n",
    "        \n",
    "    ### Repeat if training ###\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(len(X)).repeat()\n",
    "\n",
    "    dataset = dataset.batch(global_batch_size).prefetch(AUTO)\n",
    "\n",
    "    ### make it distributed  ###\n",
    "    dist_dataset = strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "    return dist_dataset\n",
    "    \n",
    "    \n",
    "train_dist_dataset = create_dist_dataset(X_train, y_train, True)\n",
    "val_dist_dataset   = create_dist_dataset(X_val)\n",
    "test_dist_dataset  = create_dist_dataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def create_model_and_optimizer():\n",
    "    with strategy.scope():\n",
    "        transformer_layer = TFRobertaModel.from_pretrained(PRETRAINED_MODEL)                \n",
    "        model = build_model(transformer_layer)\n",
    "        optimizer_transformer = Adam(learning_rate=LR_TRANSFORMER)\n",
    "        optimizer_head = Adam(learning_rate=LR_HEAD)\n",
    "    return model, optimizer_transformer, optimizer_head\n",
    "\n",
    "\n",
    "def build_model(transformer):\n",
    "    inp = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    # Huggingface transformers have multiple outputs, embeddings are the first one\n",
    "    # let's slice out the first position, the paper says its not worse than pooling\n",
    "    x = transformer(inp)[0][:, 0, :]  \n",
    "    x = Dropout(0.5)(x)\n",
    "    ### note, adding the name to later identify these weights for different LR\n",
    "    out = Dense(1, activation='sigmoid', name='custom_head')(x)\n",
    "    model = Model(inputs=[inp], outputs=[out])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model, optimizer_transformer, optimizer_head = create_model_and_optimizer()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_losses_and_metrics():\n",
    "    with strategy.scope():\n",
    "        loss_object = tf.keras.losses.BinaryCrossentropy(\n",
    "            reduction=tf.keras.losses.Reduction.NONE, \n",
    "            from_logits=False)\n",
    "\n",
    "        def compute_loss(labels, predictions):\n",
    "            per_example_loss = loss_object(labels, predictions)\n",
    "            loss = tf.nn.compute_average_loss(\n",
    "                per_example_loss, global_batch_size = global_batch_size)\n",
    "            return loss\n",
    "\n",
    "        train_accuracy_metric = tf.keras.metrics.AUC(name='training_AUC')\n",
    "\n",
    "    return compute_loss, train_accuracy_metric\n",
    "\n",
    "\n",
    "def train(train_dist_dataset, val_dist_dataset=None, y_val=None,\n",
    "          total_steps=2000, validate_every=200):\n",
    "    best_weights, history = None, []\n",
    "    step = 0\n",
    "    ### Training loop ###\n",
    "    for tensor in train_dist_dataset:\n",
    "        distributed_train_step(tensor) \n",
    "        step+=1\n",
    "\n",
    "        if (step % validate_every == 0):   \n",
    "            ### Print train metrics ###  \n",
    "            train_metric = train_accuracy_metric.result().numpy()\n",
    "            print(\"Step %d, train AUC: %.5f\" % (step, train_metric))   \n",
    "            \n",
    "            ### Test loop with exact AUC ###\n",
    "            if val_dist_dataset:\n",
    "                val_metric = roc_auc_score(y_val, predict(val_dist_dataset))\n",
    "                print(\"Step %d,   val AUC: %.5f\" %  (step,val_metric))   \n",
    "                \n",
    "                # save weights if it is the best yet\n",
    "                history.append(val_metric)\n",
    "                if history[-1] == max(history):\n",
    "                    best_weights = model.get_weights()\n",
    "\n",
    "            ### Reset (train) metrics ###\n",
    "            train_accuracy_metric.reset_states()\n",
    "            \n",
    "        if step  == total_steps:\n",
    "            break\n",
    "    \n",
    "    ### Restore best weighths ###\n",
    "    model.set_weights(best_weights)\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(data):\n",
    "    strategy.experimental_run_v2(train_step, args=(data,))\n",
    "\n",
    "def train_step(inputs):\n",
    "    features, labels = inputs\n",
    "    \n",
    "    ### get transformer and head separate vars\n",
    "    # get rid of pooler head with None gradients\n",
    "    transformer_trainable_variables = [ v for v in model.trainable_variables \n",
    "                                       if (('pooler' not in v.name)  and \n",
    "                                           ('custom' not in v.name))]\n",
    "    head_trainable_variables = [ v for v in model.trainable_variables \n",
    "                                if 'custom'  in v.name]\n",
    "\n",
    "    # calculate the 2 gradients ( note persistent, and del)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        predictions = model(features, training=True)\n",
    "        loss = compute_loss(labels, predictions)\n",
    "    gradients_transformer = tape.gradient(loss, transformer_trainable_variables)\n",
    "    gradients_head = tape.gradient(loss, head_trainable_variables)\n",
    "    del tape\n",
    "        \n",
    "    ### make the 2 gradients steps\n",
    "    optimizer_transformer.apply_gradients(zip(gradients_transformer, \n",
    "                                              transformer_trainable_variables))\n",
    "    optimizer_head.apply_gradients(zip(gradients_head, \n",
    "                                       head_trainable_variables))\n",
    "\n",
    "    train_accuracy_metric.update_state(labels, predictions)\n",
    "\n",
    "\n",
    "\n",
    "def predict(dataset):  \n",
    "    predictions = []\n",
    "    for tensor in dataset:\n",
    "        predictions.append(distributed_prediction_step(tensor))\n",
    "    ### stack replicas and batches\n",
    "    predictions = np.vstack(list(map(np.vstack,predictions)))\n",
    "    return predictions\n",
    "\n",
    "@tf.function\n",
    "def distributed_prediction_step(data):\n",
    "    predictions = strategy.experimental_run_v2(prediction_step, args=(data,))\n",
    "    return strategy.experimental_local_results(predictions)\n",
    "\n",
    "def prediction_step(inputs):\n",
    "    features = inputs  # note datasets used in prediction do not have labels\n",
    "    predictions = model(features, training=False)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "compute_loss, train_accuracy_metric = define_losses_and_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train(train_dist_dataset, val_dist_dataset, y_val,\n",
    "      TOTAL_STEPS_STAGE1, VALIDATE_EVERY_STAGE1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# decrease LR for second stage in the head\n",
    "optimizer_head.learning_rate.assign(1e-4)\n",
    "\n",
    "# split validation data into train test\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_val, y_val, test_size = 0.1)\n",
    "\n",
    "# make a datasets\n",
    "train_dist_dataset = create_dist_dataset(X_train, y_train, training=True)\n",
    "val_dist_dataset = create_dist_dataset(X_val, y_val)\n",
    "\n",
    "# train again\n",
    "train(train_dist_dataset, val_dist_dataset, y_val,\n",
    "      total_steps = TOTAL_STEPS_STAGE2, \n",
    "      validate_every = VALIDATE_EVERY_STAGE2)  # not validating but printing now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sub_df['toxic'] = predict(test_dist_dataset)[:,0]\n",
    "sub_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
