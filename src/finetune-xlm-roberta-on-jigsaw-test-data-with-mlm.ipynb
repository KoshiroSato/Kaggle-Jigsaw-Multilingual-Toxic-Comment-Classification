{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/riblidezso/finetune-xlm-roberta-on-jigsaw-test-data-with-mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c4ad26dd-8d2a-43e9-a3ea-0a4342a16e52",
    "_uuid": "07d7bc95-0377-4db3-aa05-2bd6cf989e78"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import transformers\n",
    "from transformers import TFAutoModelWithLMHead, AutoTokenizer\n",
    "import logging\n",
    "# no extensive logging \n",
    "logging.getLogger().setLevel(logging.NOTSET)\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "TOTAL_STEPS = 2000\n",
    "EVALUATE_EVERY = 200\n",
    "LR =  1e-5\n",
    "\n",
    "PRETRAINED_MODEL = 'jplu/tf-xlm-roberta-large'\n",
    "train_df = pd.read_csv('../input/jigsaw-train-translated-yandex-api/train_yandex.csv')\n",
    "test_df = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "86a303dd-5d1d-4d3f-bf28-4a02b6816e9b",
    "_uuid": "5a5fd45e-8f97-44d8-95f6-29cc8563dc3d"
   },
   "outputs": [],
   "source": [
    "def connect_to_TPU():\n",
    "    \"\"\"Detect hardware, return appropriate distribution strategy\"\"\"\n",
    "    try:\n",
    "        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "        # set: this is always the case on Kaggle.\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    else:\n",
    "        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "\n",
    "    global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "    return tpu, strategy, global_batch_size\n",
    "\n",
    "\n",
    "tpu, strategy, global_batch_size = connect_to_TPU()\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "900f2d01-b87c-42e7-8cc2-8dcba95ffa54",
    "_uuid": "7d5605a3-df77-4574-9899-89529d0a6e61"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])\n",
    "    \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "X_train = regular_encode(train_df.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "X_test = regular_encode(test_df.content.values, tokenizer, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mlm_input_and_labels(X):\n",
    "    # 15% BERT masking\n",
    "    inp_mask = np.random.rand(*X.shape)<0.15 \n",
    "    # do not mask special tokens\n",
    "    inp_mask[X<=2] = False\n",
    "    # set targets to -1 by default, it means ignore\n",
    "    labels =  -1 * np.ones(X.shape, dtype=int)\n",
    "    # set labels for masked tokens\n",
    "    labels[inp_mask] = X[inp_mask]\n",
    "    \n",
    "    # prepare input\n",
    "    X_mlm = np.copy(X)\n",
    "    # set input to [MASK] which is the last token for the 90% of tokens\n",
    "    # this means leaving 10% unchanged\n",
    "    inp_mask_2mask = inp_mask  & (np.random.rand(*X.shape)<0.90)\n",
    "    X_mlm[inp_mask_2mask] = 250001  # mask token is the last in the dict\n",
    "\n",
    "    # set 10% to a random token\n",
    "    inp_mask_2random = inp_mask_2mask  & (np.random.rand(*X.shape) < 1/9)\n",
    "    X_mlm[inp_mask_2random] = np.random.randint(3, 250001, inp_mask_2random.sum())\n",
    "    \n",
    "    return X_mlm, labels\n",
    "\n",
    "\n",
    "# use train and test data for mlm\n",
    "X_train_mlm = np.vstack([X_test, X_train])\n",
    "# masks and labels\n",
    "X_train_mlm, y_train_mlm = prepare_mlm_input_and_labels(X_train_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "757b2c93-9c07-441d-ba2e-47b8607b3795",
    "_uuid": "8b748024-314d-4256-b67b-0bc71ecac19d"
   },
   "outputs": [],
   "source": [
    "def create_dist_dataset(X, y=None, training=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "    ### Add y if present ###\n",
    "    if y is not None:\n",
    "        dataset_y = tf.data.Dataset.from_tensor_slices(y)\n",
    "        dataset = tf.data.Dataset.zip((dataset, dataset_y))\n",
    "        \n",
    "    ### Repeat if training ###\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(len(X)).repeat()\n",
    "\n",
    "    dataset = dataset.batch(global_batch_size).prefetch(AUTO)\n",
    "\n",
    "    ### make it distributed  ###\n",
    "    dist_dataset = strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "    return dist_dataset\n",
    "    \n",
    "    \n",
    "train_dist_dataset = create_dist_dataset(X_train_mlm, y_train_mlm, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3ba0b8c0-3e4c-4afe-8766-844c6b6a8c0f",
    "_uuid": "9f6161c2-a256-4285-9a0e-f23700f4b195"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def create_mlm_model_and_optimizer():\n",
    "    with strategy.scope():\n",
    "        model = TFAutoModelWithLMHead.from_pretrained(PRETRAINED_MODEL)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "mlm_model, optimizer = create_mlm_model_and_optimizer()\n",
    "mlm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "20b20e6a-16c4-475f-92ef-2a7809d21621",
    "_uuid": "50c766ac-68bd-4884-ba54-657f5d385786"
   },
   "outputs": [],
   "source": [
    "def define_mlm_loss_and_metrics():\n",
    "    with strategy.scope():\n",
    "        mlm_loss_object = masked_sparse_categorical_crossentropy\n",
    "\n",
    "        def compute_mlm_loss(labels, predictions):\n",
    "            per_example_loss = mlm_loss_object(labels, predictions)\n",
    "            loss = tf.nn.compute_average_loss(\n",
    "                per_example_loss, global_batch_size = global_batch_size)\n",
    "            return loss\n",
    "\n",
    "        train_mlm_loss_metric = tf.keras.metrics.Mean()\n",
    "        \n",
    "    return compute_mlm_loss, train_mlm_loss_metric\n",
    "\n",
    "\n",
    "def masked_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    y_true_masked = tf.boolean_mask(y_true, tf.not_equal(y_true, -1))\n",
    "    y_pred_masked = tf.boolean_mask(y_pred, tf.not_equal(y_true, -1))\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true_masked,\n",
    "                                                          y_pred_masked,\n",
    "                                                          from_logits=True)\n",
    "    return loss\n",
    "\n",
    "            \n",
    "            \n",
    "def train_mlm(train_dist_dataset, total_steps=2000, evaluate_every=200):\n",
    "    step = 0\n",
    "    ### Training loop ###\n",
    "    for tensor in train_dist_dataset:\n",
    "        distributed_mlm_train_step(tensor) \n",
    "        step+=1\n",
    "\n",
    "        if (step % evaluate_every == 0):   \n",
    "            ### Print train metrics ###  \n",
    "            train_metric = train_mlm_loss_metric.result().numpy()\n",
    "            print(\"Step %d, train loss: %.2f\" % (step, train_metric))     \n",
    "\n",
    "            ### Reset  metrics ###\n",
    "            train_mlm_loss_metric.reset_states()\n",
    "            \n",
    "        if step  == total_steps:\n",
    "            break\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def distributed_mlm_train_step(data):\n",
    "    strategy.experimental_run_v2(mlm_train_step, args=(data,))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def mlm_train_step(inputs):\n",
    "    features, labels = inputs\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = mlm_model(features, training=True)[0]\n",
    "        loss = compute_mlm_loss(labels, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, mlm_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, mlm_model.trainable_variables))\n",
    "\n",
    "    train_mlm_loss_metric.update_state(loss)\n",
    "    \n",
    "\n",
    "compute_mlm_loss, train_mlm_loss_metric = define_mlm_loss_and_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d3d0e97f-d7d4-4d3f-8c6e-925669565248",
    "_uuid": "b30a95a7-ae7e-40e8-846b-9d31bfe997ad"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_mlm(train_dist_dataset, TOTAL_STEPS, EVALUATE_EVERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "96a7be93-0dce-4d00-92c4-d4c4cb2238f8",
    "_uuid": "5cd548e4-5756-4d92-b72c-f71cb72b395d"
   },
   "outputs": [],
   "source": [
    "mlm_model.save_pretrained('./')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
